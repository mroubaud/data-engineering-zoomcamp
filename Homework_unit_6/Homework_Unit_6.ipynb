{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3ceebc",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Install Spark and PySpark\n",
    "\n",
    "Install Spark\n",
    "Run PySpark\n",
    "Create a local spark session\n",
    "Execute spark.version.\n",
    "What's the output?\n",
    "\n",
    "**3.3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9bd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4a0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/24 11:25:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/24 11:25:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3e4c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57a54f",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "* 1MB\n",
    "* **6MB**\n",
    "* 25MB\n",
    "* 87MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e751245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-24 12:45:35--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.155.128.187, 18.155.128.46, 18.155.128.6, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.155.128.187|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18344035 (17M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘fhv_tripdata_2019-10.parquet’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  17.49M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-02-24 12:45:35 (162 MB/s) - ‘fhv_tripdata_2019-10.parquet’ saved [18344035/18344035]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f43ef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>None</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
       "0               B00009 2019-10-01 00:23:00 2019-10-01 00:35:00         264.0   \n",
       "1               B00013 2019-10-01 00:11:29 2019-10-01 00:13:22         264.0   \n",
       "2               B00014 2019-10-01 00:11:43 2019-10-01 00:37:20         264.0   \n",
       "3               B00014 2019-10-01 00:56:29 2019-10-01 00:57:47         264.0   \n",
       "4               B00014 2019-10-01 00:23:09 2019-10-01 00:28:27         264.0   \n",
       "\n",
       "   DOlocationID SR_Flag Affiliated_base_number  \n",
       "0         264.0    None                 B00009  \n",
       "1         264.0    None                 B00013  \n",
       "2         264.0    None                 B00014  \n",
       "3         264.0    None                 B00014  \n",
       "4         264.0    None                 B00014  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_parquet('fhv_tripdata_2019-10.parquet')\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a9d5934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime',\n",
       "       'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cbc8c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897856 entries, 0 to 1897855\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype         \n",
      "---  ------                  -----         \n",
      " 0   dispatching_base_num    object        \n",
      " 1   pickup_datetime         datetime64[ns]\n",
      " 2   dropOff_datetime        datetime64[ns]\n",
      " 3   PUlocationID            float64       \n",
      " 4   DOlocationID            float64       \n",
      " 5   SR_Flag                 object        \n",
      " 6   Affiliated_base_number  object        \n",
      "dtypes: datetime64[ns](2), float64(2), object(3)\n",
      "memory usage: 101.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68bc8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .parquet('fhv_tripdata_2019-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4b754d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|       264.0|       264.0|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|       264.0|       264.0|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|       264.0|       264.0|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|       264.0|       264.0|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|       264.0|       264.0|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|       129.0|       129.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|        57.0|        57.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|       173.0|       173.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|       226.0|       226.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|        56.0|        56.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|        82.0|        82.0|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|       264.0|        71.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|       264.0|        39.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|       264.0|       188.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|       264.0|        91.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|       264.0|        71.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|       264.0|        35.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|       264.0|        61.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|       264.0|       198.0|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|       264.0|       264.0|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30e27169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: double (nullable = true)\n",
      " |-- DOlocationID: double (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8be955fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no lo usé\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.DoubleType(), True),\n",
    "    types.StructField('DOlocationID', types.DoubleType(), True),\n",
    "    types.StructField('SR_Flag', types.IntegerType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e68f9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .parquet('fhv_tripdata_2019-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "575b7398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: double (nullable = true)\n",
      " |-- DOlocationID: double (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b24d0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|       264.0|       264.0|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|       264.0|       264.0|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|       264.0|       264.0|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|       264.0|       264.0|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|       264.0|       264.0|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|       129.0|       129.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|        57.0|        57.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|       173.0|       173.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|       226.0|       226.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|        56.0|        56.0|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|        82.0|        82.0|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|       264.0|        71.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|       264.0|        39.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|       264.0|       188.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|       264.0|        91.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|       264.0|        71.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|       264.0|        35.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|       264.0|        61.0|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|       264.0|       198.0|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|       264.0|       264.0|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "49b4dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1533aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"fhv/2019/10/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49761f1",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "* 108,164\n",
    "* 12,856\n",
    "* 452,470\n",
    "* **62,610**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "151635d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1d043b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1897856|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "22cca824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62629|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    ((MONTH(pickup_datetime) = 10) AND (DAY(pickup_datetime) = 15) AND (YEAR(pickup_datetime) = 2019))\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e56bfc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62629"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2019-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2ab72307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62629|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2019-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32472c0b",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "* 631,152.50 Hours\n",
    "* 243.44 Hours\n",
    "* 7.68 Hours\n",
    "* **3.32 Hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88f225ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_duration = df\\\n",
    "                    .withColumn(\"pickup_date\", F.to_date('pickup_datetime'))\\\n",
    "                    .withColumn(\"travel_duration\", (F.col('dropOff_datetime')- F.col('pickup_datetime'))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6607f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_duration_2 = df\\\n",
    "                    .withColumn(\"pickup_date\", F.to_date('pickup_datetime'))\\\n",
    "                    .withColumn(\"travel_duration\", (F.col('dropOff_datetime').cast(\"long\") - F.col('pickup_datetime').cast(\"long\"))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3decdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 159:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   travel_duration|\n",
      "+------------------+\n",
      "| 47.56666666666667|\n",
      "| 7.766666666666667|\n",
      "|              39.0|\n",
      "|11.466666666666667|\n",
      "|              38.0|\n",
      "|15.683333333333334|\n",
      "|31.183333333333334|\n",
      "|              25.0|\n",
      "|17.316666666666666|\n",
      "| 38.53333333333333|\n",
      "|36.733333333333334|\n",
      "|21.716666666666665|\n",
      "|               3.1|\n",
      "| 6.333333333333333|\n",
      "| 4.466666666666667|\n",
      "|16.616666666666667|\n",
      "|21.066666666666666|\n",
      "|17.983333333333334|\n",
      "|              13.5|\n",
      "|              3.25|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_time_duration_2.select(\"travel_duration\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e62ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 162:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|pickup_date|max(travel_duration)|\n",
      "+-----------+--------------------+\n",
      "| 2019-10-28|          3.786915E7|\n",
      "| 2019-10-11|          3.786915E7|\n",
      "| 2019-10-31|          5260346.45|\n",
      "| 2019-10-01|   4207681.683333334|\n",
      "| 2019-10-17|            527640.0|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_time_duration_2.select('pickup_date','travel_duration')\\\n",
    "                .groupBy('pickup_date')\\\n",
    "                .max('travel_duration')\\\n",
    "                .orderBy('max(travel_duration)', ascending = False)\\\n",
    "                .limit(5)\\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bd0df6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_duration_2.createOrReplaceTempView('trips_data_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5ba5bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 256:======================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------------+\n",
      "|    pickup_datetime|max(datediff(dropOff_datetime, pickup_datetime))|\n",
      "+-------------------+------------------------------------------------+\n",
      "|2019-10-28 09:00:00|                                           26298|\n",
      "|2019-10-11 18:00:00|                                           26298|\n",
      "|2019-10-31 23:46:33|                                            3654|\n",
      "|2019-10-01 21:43:42|                                            2922|\n",
      "|2019-10-17 14:00:00|                                             367|\n",
      "+-------------------+------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime,\n",
    "    max(DATEDIFF(dropOff_datetime, pickup_datetime))\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY \n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce58bf",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "User Interface\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "* 80\n",
    "* 443\n",
    "* **4040**\n",
    "* 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f6f1d",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "* East Chelsea\n",
    "* **Jamaica Bay**\n",
    "* Union Sq\n",
    "* Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34f97ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-24 12:40:59--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240224T124059Z&X-Amz-Expires=300&X-Amz-Signature=c891596a95e1f8da2aa9e078b0bb988c0dd5c2a46041593be276ad40d04cb385&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-24 12:40:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240224T124059Z&X-Amz-Expires=300&X-Amz-Signature=c891596a95e1f8da2aa9e078b0bb988c0dd5c2a46041593be276ad40d04cb385&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-02-24 12:41:00 (51.0 MB/s) - ‘taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "06cc1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_zone = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "79906301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 265 entries, 0 to 264\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   LocationID    265 non-null    int64 \n",
      " 1   Borough       265 non-null    object\n",
      " 2   Zone          264 non-null    object\n",
      " 3   service_zone  263 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pandas_zone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "943b1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "           .option(\"header\", \"true\") \\\n",
    "           .csv('taxi_zone_lookup.csv')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4910acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "153c29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a1e529c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_zones = types.StructType([\n",
    "    types.StructField('LocationID', types.LongType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0ef8ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "           .option(\"header\", \"true\") \\\n",
    "           .schema(schema_zones) \\\n",
    "           .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "267f949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: long (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8f803c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d5ab8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('trips_data_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d4811f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 235:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+\n",
      "|PULocationID|LocationID|         Zone|\n",
      "+------------+----------+-------------+\n",
      "|        98.0|        98|Fresh Meadows|\n",
      "|        98.0|        98|Fresh Meadows|\n",
      "|        98.0|        98|Fresh Meadows|\n",
      "|        98.0|        98|Fresh Meadows|\n",
      "|        98.0|        98|Fresh Meadows|\n",
      "+------------+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH trips as (\n",
    "    SELECT *\n",
    "    FROM trips_data\n",
    "), \n",
    "trips_zones as (\n",
    "    SELECT *\n",
    "    FROM trips_data_zones\n",
    ")\n",
    "SELECT \n",
    "    trips.PULocationID,\n",
    "    trips_zones.LocationID,\n",
    "    trips_zones.Zone\n",
    "FROM trips\n",
    "INNER JOIN trips_zones\n",
    "    on trips.PULocationID = trips_zones.LocationID\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "92e95f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 244:=========>                                               (1 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+\n",
      "|concat(Zone, -, Borough)|count(1)|\n",
      "+------------------------+--------+\n",
      "|      Jamaica Bay-Queens|       1|\n",
      "|    Governor's Island...|       2|\n",
      "|    Green-Wood Cemete...|       5|\n",
      "|    Broad Channel-Queens|       8|\n",
      "|    Highbridge Park-M...|      14|\n",
      "+------------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CONCAT(trips_zones_pup.Zone, \"-\", trips_zones_pup.Borough),\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    trips_data trips LEFT JOIN trips_data_zones trips_zones_pup ON trips.PULocationID = trips_zones_pup.LocationID\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6475f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 250:===============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|PULocationID|count(1)|\n",
      "+------------+--------+\n",
      "|         2.0|       1|\n",
      "|       105.0|       2|\n",
      "|       111.0|       5|\n",
      "|        30.0|       8|\n",
      "|       120.0|      14|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    PULocationID, \n",
    "    COUNT(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY \n",
    "    2\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d084d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
